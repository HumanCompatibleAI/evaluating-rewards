# Copyright 2020 Adam Gleave
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#            http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Classes and methods to help interpret reward functions."""

import math
from typing import NamedTuple, Sequence

import gym
from imitation.util import rollout
import matplotlib.pyplot as plt
import numpy as np
from stable_baselines.common import policies, vec_env


# TODO(adam): consider making rollout.Transitions a dataclass and subclassing?
class RenderedTransitions(NamedTuple):
    """A batch of obs-act-obs-rew-done transitions.

    Usually generated by combining and processing several Trajectories via
    `flatten_trajectory()`.

    Attributes:
        obs: Previous observations. Shape: (batch_size, ) + observation_shape.
            The i'th observation `obs[i]` in this array is the observation seen
            by the agent when choosing action `act[i]`.
        act: Actions. Shape: (batch_size, ) + action_shape.
        next_obs: New observation. Shape: (batch_size, ) + observation_shape.
            The i'th observation `next_obs[i]` in this array is the observation
            after the agent has taken action `act[i]`.
        rew: Reward. Shape: (batch_size, ).
            The reward `rew[i]` at the i'th timestep is received after the agent has
            taken action `act[i]`.
        done: Boolean array indicating episode termination. Shape: (batch_size, ).
            `done[i]` is true iff `next_obs[i]` the last observation of an episode.
        imgs: Image. Shape: (batch_size, ) + image_shape.
    """

    obs: np.ndarray
    acts: np.ndarray
    next_obs: np.ndarray
    rews: np.ndarray
    dones: np.ndarray
    imgs: np.ndarray


class RenderWrapper(gym.Wrapper):
    """Wraps a Gym environment, adding rendered images to the info dict under key 'img'.

    Note this does not render the initial observation (there is no info dict returned by
    `reset()`). For interpretability this is of limited importance since the reward of initial
    state(s) will not change the optimal policy.
    """

    def step(self, action):
        ob, rew, done, info = self.env.step(action)
        img = self.env.render(mode="rgb_array")
        info["img"] = img
        return ob, rew, done, info

    def reset(self, **kwargs):
        return self.env.reset(**kwargs)


def rendered_rollout(
    venv: vec_env.VecEnv,
    policy: policies.BasePolicy,
    n_timesteps: int,
    *,
    truncate: bool = True,
    **kwargs,
) -> RenderedTransitions:
    """Rollout `policy` on `venv` for `n_timesteps`.

    Similar to `imitation.util.rollout.generate_transitions`, but returns `RenderedTransitions`,
    including a rendered representation of each state in the `imgs` attribute.

    Args:
         venv: A vector environment to rollout in.
         policy: The policy to rollout.
         n_timesteps: The number of timesteps of data to collect.
         truncate: if True, return exactly n_timesteps; otherwise, it is a lower bound.
         kwargs: Passed through to `rollout.generate_trajectories`.
    """
    trajs = rollout.generate_trajectories(
        policy, venv, sample_until=rollout.min_timesteps(n_timesteps), **kwargs
    )
    transitions = rollout.flatten_trajectories(trajs)
    imgs = []
    for traj in trajs:
        imgs += [info["img"] for info in traj.infos]
    if truncate and n_timesteps is not None:
        transitions = rollout.Transitions(*(arr[:n_timesteps] for arr in transitions))
        imgs = imgs[:n_timesteps]

    imgs = np.array(imgs)
    assert len(imgs) == len(transitions.obs)
    return RenderedTransitions(**transitions._asdict(), imgs=imgs)


# TODO(adam): maybe want something more extreme than quantile sampling?
# e.g. top 1%, top 5%, top 10%; or particular S.D.s of reward.
def quantile_sample(preds: np.ndarray, num_samples: int = 10) -> Sequence[int]:
    """Sample `num_samples` quantiles from preds."""
    num_points = len(preds)
    if num_samples > num_points:
        raise ValueError("num_samples must be less than the number of datapoints.")

    sorted_idxs = np.argsort(preds)
    subset = np.linspace(0, num_points - 1, num=num_samples)
    subset = np.ceil(subset).astype(np.int)
    # TODO(adam): is this actually guaranteed not to clash?
    assert len(np.unique(subset)) == len(subset)

    return sorted_idxs[subset]


def plot_renders(
    imgs: np.ndarray, reward_preds: np.ndarray, ncols: int = 3, cell_size: float = 5.0
):
    """Plots an array of images."""
    n_imgs = imgs.shape[0]
    nrows = math.ceil(n_imgs / ncols)
    figsize = (cell_size * nrows, cell_size * ncols)
    _, axs = plt.subplots(nrows, ncols, figsize=figsize, sharex=True, sharey=True)
    for i, img in enumerate(imgs):
        ax = axs[i // ncols][i % ncols]
        ax.imshow(img)
        ax.set_title("{:.2f}".format(reward_preds[i]))
