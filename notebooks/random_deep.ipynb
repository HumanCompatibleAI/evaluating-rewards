{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating Rewards: Comparing Randomly Generated Deep Reward Models",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pQX-iWpDxyF"
      },
      "source": [
        "In this Colab, we perform a simple test of our reward model comparison technique on randomly generated reward models.\n",
        "\n",
        "Specifically, we randomly sample two reward models $r_g(s,a,s')$ and $r_n(s,a,s')$ that we treat as the \"ground truth\" and \"noise\" reward model respectively. We also randomly sample a potential function $\\phi_n(s)$, a scale factor $\\lambda > 0$ and shift $c$.\n",
        "\n",
        "For a given reward noise magnitude $\\sigma_r$ and potential noise magnitude $\\sigma_\\phi$, the synthetically generated reward model is:\n",
        "  $$r_o(s,a,s') = \\lambda (r_g(s,a,s') + \\sigma_r\\cdot r_n(s,a,s') + \\sigma_\\phi(\\gamma \\phi_n(s') - \\phi_n(s))) + c,$$\n",
        "where $\\gamma$ is the discount factor.\n",
        "\n",
        "We then compare the constructed reward model $r_o$ to the ground truth $r_g$. Concretely, we search over the space of reward functions that are equivalent to $r_o$ -- will induce the same optimal policy as $r_o$ under arbitrary transition dynamics -- to find the one closest to $r_g$. The permissible transformations to $r_o$ are positive affine transformations (shifting and positive scaling) and addition of a potential function.\n",
        "\n",
        "Note that if the reward noise magnitude is set to $\\sigma_r = 0$, then $r_o$ can be transformed to exactly match $r_g$. As $\\sigma_r$ increases, it is not usually possible to match $r_g$ directly, and the magnitude of the error should increase proportionally to $r_n$. By contrast, changes in $\\lambda$, $c$, and $\\sigma_\\phi$ do not change the *minimum* distance -- but can make the optimization problem easier (when closer to the identity) or harder (when further away).\n",
        "\n",
        "We perform this experiment for a variety of values of the reward noise magnitude $\\sigma_r$ and potential noise $\\sigma_\\phi$. We use the standard $\\ell_2$ distance metric, on a transition distribution induced by randomly taking actions in two simple environments: a \"point mass\" environment where actions produce forces on a point mass with position and velocity, and a mock 5D environment where states are uniformly sampled in $[0,1]$ and actions sampled from $[0,1]$ are added to the state (clipped to stay in $[0,1]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AmgOzsDr-gm1"
      },
      "source": [
        "# Setup: Imports and Environment Creation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NBxahWcu8yg",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import tensorflow as tf\n",
        "\n",
        "from imitation.util import rollout\n",
        "\n",
        "from evaluating_rewards.envs import point_mass\n",
        "from evaluating_rewards.experiments import datasets\n",
        "from evaluating_rewards.experiments import synthetic\n",
        "from evaluating_rewards.experiments import util\n",
        "from evaluating_rewards.experiments import visualize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AVx94rb4HYGH",
        "colab": {}
      },
      "source": [
        "env_uniform = datasets.dummy_env_and_dataset(dims=5)  # 5-dimensional uniform state distribution\n",
        "env_pm = datasets.make_pm()  # PointMass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eiGWpI2-9TdF"
      },
      "source": [
        "# Experiment: Comparing noisy models\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "IKr2eIeZ9ZRN",
        "colab": {}
      },
      "source": [
        "#@title Helper Methods\n",
        "def run_compare_synthetic(reps=3, **kwargs):\n",
        "  dfs = []\n",
        "  metrics = []\n",
        "  for _ in range(reps):\n",
        "    with util.fresh_sess(intra_op=12, inter_op=12):\n",
        "      df, metric = synthetic.compare_synthetic(**kwargs)\n",
        "    dfs.append(df)\n",
        "    metrics.append(metric)\n",
        "  return dfs, metrics\n",
        "\n",
        "\n",
        "def plot_shaping_comparison(dfs, **kwargs):\n",
        "  fig, axs = plt.subplots(1, len(dfs), figsize=(16, 4), squeeze=False)\n",
        "  longforms = []\n",
        "  for df, ax in zip(dfs, axs[0]):\n",
        "    longform = visualize.plot_shaping_comparison(df, ax=ax)\n",
        "    longforms.append(longform)\n",
        "  return longforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WNfTvS6gBYoA"
      },
      "source": [
        "The below method runs a simple version of the experiment described above. It tests two different reward model architectures: a linear model, and two-hidden layer model. The reward noise magnitude ranges between $0.0$ and $1.0$, while the potential noise ranges from $0.0$ to $10.0$. Note that since the potential shaping term is the *difference* between the potential at two (usually near-by) states, the potential shaping tends to be smaller -- this is why the scale is increased for the potential noise.\n",
        "\n",
        "The graphs plot the intrinsic distance and shaping magnitude in solid and dashed lines respectively. The intrinsic distance is the minimal distance between the set of reward functions equivalent to $r_o$ and the target reward $r_t$. The shaping magnitude is the average size of the potential shaping added. The x-axis shows the reward noise $n_r$ and the different colors show the potential noise $n_\\phi$ (darker is larger)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "BGz0J5PqmlPh",
        "colab": {}
      },
      "source": [
        "#@title Run Experiment\n",
        "def compare_environment_architecture(**kwargs):\n",
        "  envs = {'pm': env_pm, 'uniform': env_uniform}\n",
        "  layers = {\n",
        "      'linear': {\n",
        "          'reward_hids': [], \n",
        "          'dataset_potential_hids': [], \n",
        "          'model_potential_hids': [],\n",
        "          'learning_rate': 1e-1\n",
        "      },\n",
        "      'twolayer': {\n",
        "          'reward_hids': [32, 32],\n",
        "          'dataset_potential_hids': [32, 32],\n",
        "          'model_potential_hids': [32, 32],\n",
        "          'learning_rate': 1e-2,\n",
        "      }\n",
        "  }\n",
        "  res = {}\n",
        "  for env_name, env in envs.items():\n",
        "    for arch_name, arch in layers.items():\n",
        "      print(env_name, arch_name)\n",
        "      res[f'{env_name}_{arch_name}'] = run_compare_synthetic(**arch, **env,\n",
        "                                                             potential_noise=np.arange(0.0, 10.0, 2.0),\n",
        "                                                             **kwargs)\n",
        "  return res\n",
        "\n",
        "env_arch_comparisons = compare_environment_architecture()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PynYMgquYDCY",
        "colab": {}
      },
      "source": [
        "#@title Plot Results\n",
        "for k, (dfs, metrics) in env_arch_comparisons.items():\n",
        "  plot_shaping_comparison(dfs)\n",
        "  plt.title(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9SAiBOv2DQay"
      },
      "source": [
        "Observe that in all cases, the intrisinc distance (solid line) is largely unaffetced by the potential shaping (line color). This is as expected, since adding potential shaping does not change the equivalence class (but may make the optimization problem harder).\n",
        "\n",
        "Furthermore, the intrinsic distance increases monotonically with reward noise, and in most cases has an approximately linear relationship as expected.\n",
        "\n",
        "For the potential shaping, observe that larger potential noises (darker colors) produce larger potential shaping. In other words, if we add more potential noise, the learned potential tends to also be larger, as expected. The potential shaping magnitude varies with reward noise, but there is no clear pattern. Intuitively, since the reward noise we're adding may itself include a potential term, this could increase the potential shaping term we need to add or decrease it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D6rg9jXS-BPo"
      },
      "source": [
        "# Experiment: Summary Statistics\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XhZMPutOELPS"
      },
      "source": [
        "We previously mentioned the potential noise was chosen to be a 10x greater scale than the reward noise, due to a smaller magnitude potential shaping. This section computes summary statistics (mean, standard deviation, etc) of a randomly initialized reward model and potential shaping.\n",
        "\n",
        "We observe a similar variance in the reward (`'reward'`) and potential shaping (`'old_potential'`, `'new_potential'`). However, on PointMass, the magnitude of the potential difference (`'shaping') is much lower (by a factor of 200x). This is due to the dynamics causing the next state to be close to the current state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2yZt5uf-DgQ",
        "colab": {}
      },
      "source": [
        "#@title Compute summary statistics on dataset\n",
        "def summary(dataset_generator, observation_space, action_space, batch_size=4096):\n",
        "  dataset = next(dataset_generator(batch_size, batch_size))\n",
        "  with util.fresh_sess():\n",
        "    summary_stats = synthetic.summary_stats(observation_space, action_space, dataset)\n",
        "  return summary_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MnHruIzyFOpV",
        "colab": {}
      },
      "source": [
        "#@title 5D uniform environment\n",
        "summary(**env_uniform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NxLUf1xsFVT1",
        "colab": {}
      },
      "source": [
        "#@title PointMass environment\n",
        "summary(**env_pm)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
