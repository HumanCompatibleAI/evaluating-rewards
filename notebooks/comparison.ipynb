{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating Rewards: Comparing Models in PointMass",
      "version": "0.3.2",
      "provenance": [
        {
          "file_id": "1R-OgmVLzVk0XOm0fHDiFh-9effFFYgea",
          "timestamp": 1566926098180
        },
        {
          "file_id": "1_EqW6JGLG9Rm_ZVvLgiOGYXdj8-_cCaT",
          "timestamp": 1565020140204
        },
        {
          "file_id": "1PBTJ_54EAzYoeqXw73PQtP_lEHnrtHrf",
          "timestamp": 1563112767371
        },
        {
          "file_id": "18F98PFPFOp9ykGaJMlk0bb9TA5FkM-HC",
          "timestamp": 1562081150215
        },
        {
          "file_id": "1YBG5Bcycv9zk7-w1rQ8ThoMYyAhKyIYQ",
          "timestamp": 1561630248521
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g6Wuvmtvsle-"
      },
      "source": [
        "In this Colab, we compare reward models trained with IRL to hand-written reward functions in a simple \"point mass\" environment.\n",
        "\n",
        "In each case, we have an original reward model $r_o$ and a target $r_t$. We seek to find the reward model $r$ that is equivalent to $r_o$ and is a minimal distance from the target $r_t$. Specifically, $r$ ranges over the set of reward models that are positive affine transformations and potential shaped versions of $r_o$:\n",
        "  $$r(s,a,s') = \\lambda(r_o(s,a,s') + \\gamma \\phi(s') - \\phi(s)) + c,$$\n",
        "where $\\lambda > 0$ is a rescaling constant, $c \\in \\mathbb{R}$ a shift constant and $\\phi(s)$ a potential function.\n",
        "\n",
        "We solve the optimization problem:\n",
        "  $$\\min_{\\lambda, c, \\phi} D(r, r_t),$$\n",
        "where the distance metric $D$ is:\n",
        "  $$D(r_A, r_B) = \\mathbb{E}_{s,a,s'} \\left[(r_A(s,a,s') - r_B(s, a, s'))^2\\right]$$\n",
        "\n",
        "Note this distance metric depends on a state-action-next state distribution. In this notebook, we choose a simple distribution that samples $s$ and $a$ randomly, and then computes $s'$ by the (deterministic) dynamics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AmgOzsDr-gm1"
      },
      "source": [
        "# Setup: Imports, Environment Creation and Loading Models\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NBxahWcu8yg",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "\n",
        "import gym\n",
        "from stable_baselines.common import vec_env\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "import tensorflow as tf\n",
        "import uuid\n",
        "import xarray\n",
        "\n",
        "from imitation.policies import base\n",
        "from imitation.rewards import reward_net\n",
        "from imitation.util import rollout\n",
        "\n",
        "from evaluating_rewards import serialize\n",
        "from evaluating_rewards.envs import point_mass\n",
        "from evaluating_rewards.experiments import comparisons\n",
        "from evaluating_rewards.experiments import datasets\n",
        "from evaluating_rewards.experiments import point_mass_analysis\n",
        "from evaluating_rewards.experiments import visualize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCODOrOmu0kT",
        "colab": {}
      },
      "source": [
        "# Config\n",
        "MODEL_PATH = os.path.expanduser(\"~/output/train_adversarial\")  # saved reward models\n",
        "OUT_PATH = os.path.expanduser(\"~/output/comparison\")  # path to save results to\n",
        "env_name = 'evaluating_rewards/PointMassLineFixedHorizon-v0'\n",
        "\n",
        "HARDCODED_MODEL_TYPES = [\n",
        "  'Zero-v0',\n",
        "  'PointMassGroundTruth-v0',\n",
        "  'PointMassSparseReward-v0',\n",
        "  'PointMassSparseRewardNoCtrl-v0',\n",
        "  'PointMassDenseReward-v0',\n",
        "  'PointMassDenseRewardNoCtrl-v0',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXeC_5-5EfQA",
        "colab": {}
      },
      "source": [
        "def find_model_paths(env_name):\n",
        "  root_dir = os.path.join(MODEL_PATH, env_name.replace('/', '_'))\n",
        "  return {name: os.path.join(root_dir, name,\n",
        "                             'checkpoints', 'final', 'discrim', 'reward_net')\n",
        "          for name in os.listdir(root_dir)}\n",
        "\n",
        "def load_trained_models(venv, model_paths):\n",
        "  reward_models = collections.OrderedDict()\n",
        "\n",
        "  for k, path in model_paths.items():\n",
        "    reward_models[f'{k}_shaped'] = serialize.load_reward('imitation/BasicShapedRewardNet_shaped', path, venv)\n",
        "    reward_models[f'{k}_unshaped'] = serialize.load_reward('imitation/BasicShapedRewardNet_unshaped', path, venv)\n",
        "  \n",
        "  return reward_models\n",
        "\n",
        "def load_hardcoded_models(venv, model_types):\n",
        "  reward_models = collections.OrderedDict()\n",
        "\n",
        "  for kind in model_types:\n",
        "    reward_models[kind] = serialize.load_reward(f'evaluating_rewards/{kind}',\n",
        "                                                None, venv)\n",
        "  \n",
        "  return reward_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_BCf7pf80r1D",
        "colab": {}
      },
      "source": [
        "# Environment creation and model loading \n",
        "env = gym.make(env_name)\n",
        "venv = vec_env.DummyVecEnv([lambda: env])\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  sess = tf.Session(graph=graph)\n",
        "  with sess.as_default():\n",
        "    reward_models = load_hardcoded_models(venv, HARDCODED_MODEL_TYPES)\n",
        "    model_paths = find_model_paths(env_name)\n",
        "    reward_models.update(load_trained_models(venv, model_paths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mk8xC_IS3Qfx"
      },
      "source": [
        "# Reward model plots\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXR7HtJpPIIq",
        "colab": {}
      },
      "source": [
        "def visualize_all(env, env_name, reward_models, goal=None, density=11):\n",
        "  if goal is None:\n",
        "    goal = np.array([0.0])\n",
        "  \n",
        "  with sess.as_default():\n",
        "    rewards, _ = point_mass_analysis.evaluate_multiple_reward_models(env,\n",
        "                                                                     reward_models, \n",
        "                                                                     goal=goal, \n",
        "                                                                     density=11)\n",
        "  acc_figs = point_mass_analysis.plot_multiple_rewards(rewards,\n",
        "                                                       goal, \n",
        "                                                       zaxis='acceleration')\n",
        "  vel_figs = point_mass_analysis.plot_multiple_rewards(rewards,\n",
        "                                                      goal, \n",
        "                                                      zaxis='velocity')\n",
        "  pos_figs = point_mass_analysis.plot_multiple_rewards(rewards,\n",
        "                                                      goal, \n",
        "                                                      zaxis='position')\n",
        "\n",
        "  save_path = os.path.join(OUT_PATH, env_name.replace('/', '_'))\n",
        "  visualize.save_figs(os.path.join(save_path, 'byacc'), acc_figs.items())\n",
        "  visualize.save_figs(os.path.join(save_path, 'byvel'), vel_figs.items())\n",
        "  visualize.save_figs(os.path.join(save_path, 'bypos'), pos_figs.items())\n",
        "\n",
        "visualize_all(env, env_name, reward_models)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AdNEv7bJ3ha3"
      },
      "source": [
        "# Potential Matching\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEdQuxfg7aTB",
        "colab": {}
      },
      "source": [
        "random_policy = base.RandomPolicy(env.observation_space, env.action_space)\n",
        "random_policy_dataset = datasets.rollout_generator(env, random_policy)\n",
        "hardcoded_policy = point_mass.PointMassPolicy(env)\n",
        "hardcoded_dataset = datasets.rollout_generator(env, hardcoded_policy)\n",
        "random_model_dataset = datasets.random_generator(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XgoyzCTm7gyJ",
        "colab": {}
      },
      "source": [
        "def match_pipeline(env, env_name, original, target, name,\n",
        "                   dataset=random_model_dataset, goal=None, **kwargs):\n",
        "  with sess.as_default():\n",
        "    with graph.as_default():\n",
        "      res = point_mass_analysis.match_pipeline(env, original, target, dataset, **kwargs)\n",
        "\n",
        "  save_path = os.path.join(OUT_PATH, env_name.replace(\"/\", \"_\"))\n",
        "  visualize.save_figs(os.path.join(save_path, \"bypos\"),\n",
        "                      {name: res[\"fig\"]}.items())\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mecmrLdRf19F",
        "colab": {}
      },
      "source": [
        "hardcoded_models = set(HARDCODED_MODEL_TYPES).difference(['Zero-v0'])\n",
        "irl_models = set(reward_models.keys()).difference(HARDCODED_MODEL_TYPES)\n",
        "fit_models = {}\n",
        "for target_model in hardcoded_models:\n",
        "  fit_models[target_model] = {}\n",
        "  for irl_model in irl_models:\n",
        "    print(f\"Matching {irl_model} to {target_model}\")\n",
        "\n",
        "    res = match_pipeline(env, env_name,\n",
        "                         reward_models[irl_model],\n",
        "                         reward_models[target_model],\n",
        "                         name=f\"{irl_model}_vs_{target_model}\")\n",
        "    fit_models[target_model][irl_model] = res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EX37C1NDAm5v",
        "colab": {}
      },
      "source": [
        "with sess.as_default():\n",
        "  print(comparisons.constant_baseline(sparse_vs_gt['match'], reward_models['PointMassGroundTruth-v0'], dataset=random_model_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XBDfnPDl3obA"
      },
      "source": [
        "# State Distributions\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxqxKYzTWn2J",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "point_mass_analysis.plot_state_density(random_model_dataset, 2**16)\n",
        "plt.title('Random Model-Based')\n",
        "\n",
        "fig = plt.figure()\n",
        "point_mass_analysis.plot_state_density(random_policy_dataset, 2**16)\n",
        "plt.title('Random Policy')\n",
        "\n",
        "fig = plt.figure()\n",
        "point_mass_analysis.plot_state_density(hardcoded_dataset, 2**16)\n",
        "plt.title('Hardcoded')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
