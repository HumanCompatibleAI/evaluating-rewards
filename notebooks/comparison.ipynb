{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse comparisons between reward functions, produced by running `runners/comparison/hardcoded.sh` and `runners/comparison/learnt.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from absl import logging\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from evaluating_rewards.experiments import results\n",
    "from evaluating_rewards.experiments import visualize\n",
    "\n",
    "OUTPUT_ROOT = os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "DATA_ROOT = os.path.join(OUTPUT_ROOT, \"comparison\")\n",
    "FIGURES_ROOT = os.path.join(OUTPUT_ROOT, \"figures\")\n",
    "\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hardcoded_root_dir = os.path.join(DATA_ROOT, 'hardcoded')\n",
    "hardcoded_keys = ['source_reward_type', 'target_reward_type', 'seed']\n",
    "hardcoded_stats = {}\n",
    "for env_name in ['Hopper-v3', 'HalfCheetah-v3']:\n",
    "    hardcoded_stats[env_name] = results.load_multiple_stats(\n",
    "        hardcoded_root_dir, hardcoded_keys,\n",
    "        cfg_filter=lambda k: k['env_name'] == f'evaluating_rewards/{env_name}',\n",
    "    )\n",
    "\n",
    "for kind in ['policy', 'transition']:\n",
    "    dataset_factory = f'evaluating_rewards.experiments.datasets.random_{kind}_generator'\n",
    "    mock_env_name = f'PointMassLine_{kind}-v0'\n",
    "    hardcoded_stats[mock_env_name] = results.load_multiple_stats(\n",
    "        hardcoded_root_dir, hardcoded_keys,\n",
    "        cfg_filter=lambda k: k['env_name'] == 'evaluating_rewards/PointMassLine-v0' and \n",
    "                             k['dataset_factory']['py/function'] == dataset_factory\n",
    "    )\n",
    "\n",
    "hardcoded_res = {env_name: results.pipeline(stats) for env_name, stats in hardcoded_stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_stats = {}\n",
    "regress_res = {}\n",
    "for env_name in ['PointMassLine-v0', 'Hopper-v3', 'HalfCheetah-v3']:\n",
    "    regress_stats[env_name] = results.load_multiple_stats(\n",
    "        os.path.join(DATA_ROOT, 'train_regress'),\n",
    "        ['source_reward_path', 'target_reward_type', 'seed'],\n",
    "        cfg_filter=lambda k: k['env_name'] == f'evaluating_rewards/{env_name}' and not k['model_wrapper_kwargs'],\n",
    "    )\n",
    "    regress_res[env_name] = results.pipeline(regress_stats[env_name], figsize=(12, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences_stats = {}\n",
    "preferences_res = {}\n",
    "for env_name in ['PointMassLine-v0', 'Hopper-v3', 'HalfCheetah-v3']:\n",
    "    preferences_stats[env_name] = results.load_multiple_stats(\n",
    "        os.path.join(DATA_ROOT, 'train_preferences'),\n",
    "        ['source_reward_path', 'target_reward_type', 'seed'],\n",
    "        cfg_filter=lambda k: k['env_name'] == f'evaluating_rewards/{env_name}' and not k['model_wrapper_kwargs'],\n",
    "    )\n",
    "    preferences_res[env_name] = results.pipeline(preferences_stats[env_name], figsize=(12, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presentation_heatmaps():\n",
    "    FIG_FNS = {\n",
    "        \"PointMassLine_policy-v0\": results.point_mass_heatmaps,\n",
    "        \"PointMassLine_transition-v0\": results.point_mass_heatmaps,\n",
    "        \"PointMassLine-v0\": results.point_mass_heatmaps,\n",
    "        \"Hopper-v3\": results.hopper_heatmaps,\n",
    "    }\n",
    "\n",
    "    RES = {\n",
    "        \"hardcoded\": hardcoded_res,\n",
    "        \"regress\": regress_res,\n",
    "        \"preferences\": preferences_res,\n",
    "    }\n",
    "\n",
    "    for dirname, res in RES.items():\n",
    "        for key, fn in FIG_FNS.items():\n",
    "            if key in res:\n",
    "                figs = fn(res[key][\"loss\"][\"loss\"])\n",
    "                visualize.save_figs(os.path.join(FIGURES_ROOT, dirname, key), figs.items())\n",
    "\n",
    "presentation_heatmaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcoded_loss_norm = hardcoded[\"loss\"][\"loss\"] / hardcoded[\"loss\"][\"loss\"].loc[('evaluating_rewards/Zero-v0', slice(None), 0')]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "hardcoded_loss_norm = hardcoded_loss_norm.loc[hardcoded_loss_norm.index.get_level_values('target_reward_type') != 'evaluating_rewards/Zero-v0']\n",
    "visualize.comparison_heatmap(hardcoded_loss_norm, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
